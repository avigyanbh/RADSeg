diff --git a/application/eval/evaluate_sem_seg.py b/application/eval/evaluate_sem_seg.py
index f0d1ae5..c3f5aac 100644
--- a/application/eval/evaluate_sem_seg.py
+++ b/application/eval/evaluate_sem_seg.py
@@ -15,7 +15,7 @@ from sklearn.neighbors import BallTree
 import torch
 import torchmetrics as tm
 
-from hovsg.labels.label_constants import SCANNET_COLOR_MAP_20, SCANNET_LABELS_20
+from hovsg.labels.label_constants import SCANNET_COLOR_MAP_20, SCANNET_LABELS_20, SCANNET_LABELS_NYU_40_WO_OTHERS, SCANNET_COLOR_MAP_NYU_40_WO_OTHERS
 from hovsg.utils.eval_utils import (
     load_feature_map,
     knn_interpolation,
@@ -37,6 +37,13 @@ from hovsg.utils.metric import (
 @hydra.main(version_base=None, config_path="../../config", config_name="eval_sem_seg")
 def main(params: DictConfig):
 
+    if params.main.dataset == "scannet":
+        save_dir = os.path.join("ScanNetDataset", params.main.scene_name)
+    elif params.main.dataset == "replica":
+        save_dir = os.path.join("ReplicaDataset", params.main.scene_name[:-2] + params.main.scene_name[-1])
+    if not os.path.exists(save_dir):
+        os.mkdir(save_dir)
+
     # load CLIP model
     if params.models.clip.type == "ViT-L/14@336px":
         clip_model, _, preprocess = open_clip.create_model_and_transforms(
@@ -55,13 +62,19 @@ def main(params: DictConfig):
     clip_model.eval()
 
     # Load Feature Map
-    masked_pcd, mask_feats = load_feature_map(params.main.feature_map_path)
+    if params.main.dataset == "scannet":
+        masked_pcd, mask_feats = load_feature_map(os.path.join(params.main.feature_map_path, params.main.scene_name))
+    elif params.main.dataset == "replica":
+        masked_pcd, mask_feats = load_feature_map(os.path.join(params.main.feature_map_path, params.main.scene_name[:-2] + params.main.scene_name[-1]))
+    
     # read semantic classes
     scene_name = params.main.scene_name
     if params.main.dataset == "scannet":
-        SCANNET_LABELS_20_list = list(SCANNET_LABELS_20)
-        labels = SCANNET_LABELS_20_list
-        labels_id = list(SCANNET_COLOR_MAP_20.keys())
+        # Patch Note: we the labels from NYU40 without "OTHERS" classes for evaluation.
+        SCANNET_LABELS_40_list = list(SCANNET_LABELS_NYU_40_WO_OTHERS)
+        labels = SCANNET_LABELS_40_list
+        labels_id = list(SCANNET_COLOR_MAP_NYU_40_WO_OTHERS.keys())
+
     elif params.main.dataset == "replica":
         semantic_info_path = os.path.join(
             params.main.replica_dataset_gt_path, scene_name, "habitat", "info_semantic.json"
@@ -79,7 +92,7 @@ def main(params: DictConfig):
     # create a new pcd from the labeld pcd masks
     pcd = o3d.geometry.PointCloud()
     if params.main.dataset == "scannet":
-        colors = np.array([SCANNET_COLOR_MAP_20[i] for i in labels]) / 255.0
+        colors = np.array([SCANNET_COLOR_MAP_NYU_40_WO_OTHERS[i] for i in labels]) / 255.0
     elif params.main.dataset == "replica":
         # assign color based on labels id
         colors_map = {}
@@ -98,9 +111,10 @@ def main(params: DictConfig):
 
     # load ground truth pcd
     if params.main.dataset == "scannet":
-        pcd_gt = o3d.io.read_point_cloud(
-            os.path.join(params.main.scannet_dataset_gt_path, scene_name, f"{scene_name}_vh_clean_2.labels.ply")
-        )
+        # pcd_gt = o3d.io.read_point_cloud(
+        #     os.path.join(params.main.scannet_dataset_gt_path, scene_name, f"{scene_name}_vh_clean_2.labels.ply")
+        # )
+        pcd_gt = None
     elif params.main.dataset == "replica":
         ply_path = os.path.join(params.main.replica_dataset_gt_path, scene_name, "habitat", "mesh_semantic.ply")
         gt_pcd, gt_labels, gt_instance_pcd, gt_instance_id = read_ply_and_assign_colors_replica(
@@ -115,6 +129,7 @@ def main(params: DictConfig):
         gt_pcd.colors = o3d.utility.Vector3dVector(colors)
         # o3d.io.write_point_cloud(os.path.join(save_dir, "gt_pcd.ply"), gt_pcd)
 
+    # Patch Note: we extract the encoder predicted labels and save as torch tensors. 
     if params.main.dataset == "scannet":
         # create labels_pred
         label_pre = np.zeros((len(pcd.points), 1))
@@ -122,36 +137,29 @@ def main(params: DictConfig):
             # find the color of the point
             color = np.asarray(pcd.colors[i]) * 255.0
             # find the index of the color in the color map
-            color_map_array = np.array(list(SCANNET_COLOR_MAP_20.values()))
+            color_map_array = np.array(list(SCANNET_COLOR_MAP_NYU_40_WO_OTHERS.values()))
             color_diff = np.sum(np.abs(color_map_array - color), axis=1)
             min_diff_index = np.argmin(color_diff)
             # find the label of the point
-            label = np.array(list(SCANNET_COLOR_MAP_20.keys()))[min_diff_index]
+            label = np.array(list(SCANNET_COLOR_MAP_NYU_40_WO_OTHERS.keys()))[min_diff_index]
             label_pre[i] = label
         label_pred = torch.tensor(label_pre)
-        # create labels_gt
-        labels_gt = np.zeros((len(pcd_gt.points), 1))
-        for i in range(len(pcd_gt.points)):
-            # find the color of the point
-            color = np.asarray(pcd_gt.colors[i]) * 255.0
-            # find the index of the color in the color map
-            color_map_array = np.array(list(SCANNET_COLOR_MAP_20.values()))
-            color_diff = np.sum(np.abs(color_map_array - color), axis=1)
-            min_diff_index = np.argmin(color_diff)
-            # find the label of the point
-            label = np.array(list(SCANNET_COLOR_MAP_20.keys()))[min_diff_index]
-            labels_gt[i] = label
-        labels_gt = torch.tensor(labels_gt)
-        # concat coords and labels for predicied pcd
         coords_labels = np.zeros((len(pcd.points), 4))
         coords_labels[:, :3] = np.asarray(pcd.points)
         coords_labels[:, -1] = label_pred[:, 0]
-        coords_gt = np.zeros((len(pcd_gt.points), 4))
-        coords_gt[:, :3] = np.asarray(pcd_gt.points)
-        coords_gt[:, -1] = labels_gt[:, 0]
-        match_pc = knn_interpolation(coords_labels, coords_gt, k=5)
-        label_pred = match_pc[:, -1].reshape(-1, 1)
-        labels_gt = labels_gt.numpy()
+
+        # get coords torch tensors
+        torch_labels = torch.from_numpy(coords_labels[:, :3])
+        torch_pred_labels = torch.from_numpy(coords_labels[:, -1])
+
+        # Save as dictionary
+        torch.save(
+            dict(
+            semseg_pred_xyz=torch_labels,
+            semseg_pred_label=torch_pred_labels
+            ),
+            os.path.join(save_dir, "semseg_pred.pt")
+        )
 
     elif params.main.dataset == "replica":
         pred_labels = []
@@ -170,6 +178,30 @@ def main(params: DictConfig):
         coords_gt = np.zeros((len(gt_pcd.points), 4))
         coords_gt[:, :3] = np.asarray(gt_pcd.points)
         coords_gt[:, -1] = gt_labels[:, 0]
+
+        # Patch Note: we extract the ground truth labels for Replica here for RayFronts evaluation. 
+        # get coords torch tensors
+        torch_labels = torch.from_numpy(coords_labels[:, :3])
+        torch_pred_labels = torch.from_numpy(coords_labels[:, -1])
+        torch_gt_xyz = torch.from_numpy(coords_gt[:, :3])
+        torch_gt_labels = torch.from_numpy(coords_gt[:, -1])
+
+        # Save as dictionary
+        torch.save(
+            dict(
+            semseg_pred_xyz=torch_labels,
+            semseg_pred_label=torch_pred_labels
+            ),
+            os.path.join(save_dir, "semseg_pred.pt")
+        )
+        torch.save(
+            dict(
+            semseg_gt_xyz=torch_gt_xyz,
+            semseg_gt_label=torch_gt_labels
+            ),
+            os.path.join(save_dir, "semseg_gt.pt")
+        )
+
         # knn interpolation
         match_pc = knn_interpolation(coords_labels, coords_gt, k=5)
         pred_labels = match_pc[:, -1].reshape(-1, 1)
diff --git a/application/semantic_segmentation.py b/application/semantic_segmentation.py
index 6602bbc..6abf650 100644
--- a/application/semantic_segmentation.py
+++ b/application/semantic_segmentation.py
@@ -12,7 +12,7 @@ from hovsg.graph.graph import Graph
 @hydra.main(version_base=None, config_path="../config", config_name="semantic_segmentation")
 def main(params: DictConfig):
     # Create save directory
-    save_dir = os.path.join(params.main.save_path, params.main.dataset)
+    save_dir = os.path.join(params.main.save_path, params.main.scene_id)
     if not os.path.exists(save_dir):
         os.makedirs(save_dir, exist_ok=True)
     # Create graph
diff --git a/config/eval_sem_seg.yaml b/config/eval_sem_seg.yaml
index 526f4a1..22d3e49 100644
--- a/config/eval_sem_seg.yaml
+++ b/config/eval_sem_seg.yaml
@@ -1,14 +1,34 @@
+# For ScanNet
+hydra:
+  sweeper:
+    params:
+      main.scene_name: scene0050_02, scene0378_02, scene0231_00, scene0518_00, scene0011_00, scene0231_01, scene0011_01, scene0231_02, scene0050_00, scene0378_00, scene0050_01, scene0378_01
 main:
   device: cuda
-  dataset: scannet # scannet
+  dataset: scannet
   scene_name: scene0011_00
-  feature_map_path: /home/SOME_USERNAME/data/hovsg/scannet
-  replica_dataset_gt_path: /home/SOME_USERNAME/data/replica_v1
-  scannet_dataset_gt_path: /home/SOME_USERNAME/data/ScanNet/scans
-  # only for replica
-  replica_color_map: /home/SOME_USERNAME/HOV-SG/hovsg/labels/class_id_colors.json
+  feature_map_path: ???
+  replica_dataset_gt_path: ???
+  scannet_dataset_gt_path: ???
+
+# # For Replica
+# hydra:
+#   sweeper:
+#     params:
+#       main.scene_name: office_0,office_1,office_2,office_3,office_4,room_0,room_1,room_2
+# main:
+#   device: cuda
+#   dataset: replica
+#   scene_name: office_0
+#   feature_map_path: ???
+#   replica_dataset_gt_path: ???
+#   scannet_dataset_gt_path: ???
+#   # only for replica
+#   replica_color_map: ???
+
 models:
   clip:
     general_type: ViT-L-14
-    type:  ViT-H-14 # ViT-L/14@336px # ViT-H-14
-    checkpoint: checkpoints/laion2b_s32b_b79k.bin # checkpoints/ovseg_clipl14_9a1909.pth checkpoints/laion2b_s32b_b79k.bin
+    # type:  ViT-H-14 # ViT-L/14@336px # ViT-H-14
+    type: ViT-L/14@336px
+    checkpoint: ???
diff --git a/config/semantic_segmentation.yaml b/config/semantic_segmentation.yaml
index 02c4fed..093dcfd 100644
--- a/config/semantic_segmentation.yaml
+++ b/config/semantic_segmentation.yaml
@@ -1,31 +1,48 @@
+# For ScanNet
+hydra:
+  sweeper:
+    params:
+      main.scene_id: scene0050_02, scene0378_02, scene0231_00, scene0518_00, scene0011_00, scene0231_01, scene0011_01, scene0231_02, scene0050_00, scene0378_00, scene0050_01, scene0378_01
 main:
   device: cuda
-  dataset: replica # scannet, replica
-  scene_id: office0 # scene0011_00
-  dataset_path: /data/SOME_USERNAME/hm3dsem_walks/val/00824-Dd4bFSTQ8gi
-  save_path: /data/SOME_USERNAME/hovsg/
+  dataset: scannet
+  scene_id: scene0011_01
+  dataset_path: ???
+  save_path: ???
+
+# # For Replica
+# hydra:
+#   sweeper:
+#     params:
+#       main.scene_id: office0,office1,office2,office3,office4,room0,room1,room2
+# main:
+#   device: cuda
+#   dataset: replica
+#   scene_id: office0
+#   dataset_path: ???
+#   save_path: ???
+
 models:
   clip:
-    type:  ViT-H-14 # ViT-L/14@336px # ViT-H-14
-    checkpoint: checkpoints/laion2b_s32b_b79k.bin 
-    # checkpoint: checkpoints/ovseg_clipl14_9a1909.pth checkpoints/laion2b_s32b_b79k.bin
+    type: ViT-L/14@336px
+    checkpoint: ???
   sam:
-    checkpoint: checkpoints/sam_vit_h_4b8939.pth
-    type: vit_h
-    points_per_side: 12
+    checkpoint: ???
+    type: vit_l
+    points_per_side: 6
     pred_iou_thresh: 0.88
     points_per_batch: 144
     crop_n_layers: 0
     stability_score_thresh: 0.95
-    min_mask_region_area: 100
+    min_mask_region_area: 25
 pipeline:
-  voxel_size: 0.02
+  voxel_size: 0.05
   skip_frames: 10
   init_overlap_thresh: 0.75
   overlap_thresh_factor: 0.025
   iou_thresh: 0.05
   clip_masked_weight: 0.4418
-  clip_bbox_margin: 50 # in pixels
+  clip_bbox_margin: 25
   feature_dbscan_eps: 0.01
   max_mask_distance: 10000 # 6.4239 in meters
   min_pcd_points: 100
@@ -35,3 +52,4 @@ pipeline:
   save_intermediate_results: false
   obj_labels: HM3DSEM_LABELS
   merge_objects_graph: false
+  rgb_resolution: [640, 480]
diff --git a/hovsg/dataloader/generic.py b/hovsg/dataloader/generic.py
index 292e0a8..8cda601 100644
--- a/hovsg/dataloader/generic.py
+++ b/hovsg/dataloader/generic.py
@@ -118,6 +118,11 @@ class RGBDDataset(Dataset, ABC):
         X = (x - camera_matrix[0, 2]) * depth / camera_matrix[0, 0]
         Y = (y - camera_matrix[1, 2]) * depth / camera_matrix[1, 1]
         Z = depth
+
+        # Patch Note: If the depth image is empty, return an empty point cloud to suppress ScanNet errors
+        if Z.size == 0:
+            return o3d.geometry.PointCloud()
+
         if Z.mean() > filter_distance:
             return o3d.geometry.PointCloud()
         # convert to open3d point cloud
@@ -154,9 +159,14 @@ class RGBDDataset(Dataset, ABC):
             mask = np.array(mask)
             # create pcd from mask
             pcd_masked = self.create_pcd(mask, depth, camera_pose, mask_img=True, filter_distance=filter_distance)
+            if len(pcd_masked.points) == 0:
+                pcd_list.append(pcd_masked)
+                continue
             # using KD-Tree to find the nearest points in the point cloud
             pcd_masked = np.asarray(pcd_masked.points)
             dist, indices = full_pcd_tree.query(pcd_masked, k=1, workers=-1)
+            if (indices >= len(pcd)).sum() > 0:
+                indices = indices[indices < len(pcd)]
             pcd_masked = pcd[indices]
             pcd_mask = o3d.geometry.PointCloud()
             pcd_mask.points = o3d.utility.Vector3dVector(pcd_masked)
diff --git a/hovsg/dataloader/replica.py b/hovsg/dataloader/replica.py
index bd20962..58c2cc0 100644
--- a/hovsg/dataloader/replica.py
+++ b/hovsg/dataloader/replica.py
@@ -25,6 +25,10 @@ class ReplicaDataset(RGBDDataset):
         super(ReplicaDataset, self).__init__(cfg)
         self.root_dir = cfg["root_dir"]
         self.transforms = cfg["transforms"]
+        # Patch Note: enable resizing for Replica dataset
+        self.img_size = cfg["resize"]
+        self.resize_down_x = 1200 / self.img_size[0]
+        self.resize_down_y = 680 / self.img_size[1]
         self.depth_intrinsics, self.scale = self._load_depth_intrinsics(os.path.split(self.root_dir)[0] + "/cam_params.json")
         self.data_list = self._get_data_list()
         
@@ -46,6 +50,10 @@ class ReplicaDataset(RGBDDataset):
             # convert to Tensor
             rgb_image = self.transforms(rgb_image)
             depth_image = self.transforms(depth_image)
+        # Patch Note: enable antialiasing for resizing
+        if self.img_size is not None:
+            rgb_image = rgb_image.resize(self.img_size, Image.LANCZOS)
+            depth_image = depth_image.resize(self.img_size, Image.LANCZOS)
         return rgb_image, depth_image, pose, list(), self.depth_intrinsics
     
     def _get_data_list(self):
@@ -133,12 +141,11 @@ class ReplicaDataset(RGBDDataset):
             data = json.load(file)
             camera_params = data.get("camera")
             if camera_params:
-                w = camera_params.get("w")
-                h = camera_params.get("h")
-                fx = camera_params.get("fx")
-                fy = camera_params.get("fy")
-                cx = camera_params.get("cx")
-                cy = camera_params.get("cy")
+                # Patch Note: scale camera parameters according to the resize factor
+                fx = camera_params.get("fx") / self.resize_down_x
+                fy = camera_params.get("fy") / self.resize_down_y
+                cx = camera_params.get("cx") / self.resize_down_x
+                cy = camera_params.get("cy") / self.resize_down_y
                 scale = camera_params.get("scale")
                 # Creating the camera matrix K
                 K = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
diff --git a/hovsg/dataloader/scannet.py b/hovsg/dataloader/scannet.py
index bffabce..a613b28 100644
--- a/hovsg/dataloader/scannet.py
+++ b/hovsg/dataloader/scannet.py
@@ -26,11 +26,15 @@ class ScannetDataset(RGBDDataset):
         super(ScannetDataset, self).__init__(cfg)
         self.root_dir = cfg["root_dir"]
         self.transforms = cfg["transforms"]
-        self.rgb_intrinsics = self._load_rgb_intrinsics(self.root_dir + "intrinsic/intrinsic_color.txt")
-        self.depth_intrinsics = self._load_depth_intrinsics(self.root_dir + "intrinsic/intrinsic_depth.txt")
+        # Patch Note: enable resizing for ScanNet dataset
+        self.img_size = cfg["resize"]
+        self.resize_down_x = 1296 / self.img_size[0]
+        self.resize_down_y = 968 / self.img_size[1]
+        self.rgb_intrinsics = self._load_rgb_intrinsics(os.path.join(self.root_dir, "intrinsic/intrinsic_color.txt"))
+        self.depth_intrinsics = self._load_depth_intrinsics(os.path.join(self.root_dir, "intrinsic/intrinsic_depth.txt"))
         self.scale = 1000.0
         self.data_list = self._get_data_list()
-        
+
     def __getitem__(self, idx):
         """
         Get a data sample based on the given index.
@@ -50,7 +54,12 @@ class ScannetDataset(RGBDDataset):
             # convert to Tensor
             rgb_image = self.transforms(rgb_image)
             depth_image = self.transforms(depth_image)
-            
+        
+        # Patch Note: enable antialiasing for resizing
+        if self.img_size is not None:
+            rgb_image = rgb_image.resize(self.img_size, Image.LANCZOS)
+            # depth_image = depth_image.resize(self.img_size, Image.LANCZOS) # Depth images are not resized as ScanNet depth images are already in the desired resolution
+
         return rgb_image, depth_image, pose, self.rgb_intrinsics, self.depth_intrinsics
     
     def _get_data_list(self):
@@ -63,16 +72,14 @@ class ScannetDataset(RGBDDataset):
         rgb_data_list = []
         depth_data_list = []
         pose_data_list = []
-        rgb_data_list = os.listdir(self.root_dir + "color")
-        rgb_data_list = [self.root_dir + "color/" + x for x in rgb_data_list]
-        depth_data_list = os.listdir(self.root_dir + "depth")
-        depth_data_list = [self.root_dir + "depth/" + x for x in depth_data_list]
-        pose_data_list = os.listdir(self.root_dir + "pose")
-        pose_data_list = [self.root_dir + "pose/" + x for x in pose_data_list]
-        # sort the data list
-        rgb_data_list.sort()
-        depth_data_list.sort()
-        pose_data_list.sort()
+        # Patch Note: fixed ScanNet dataset loading ordering issue with naive list.sort(), which returns the files in string alphabetical order instead of numerical order, resulting in jumped ordering.
+        rgb_data_list = os.listdir(os.path.join(self.root_dir, "color"))
+        rgb_data_list = [os.path.join(self.root_dir, "color", f"{x}.jpg") for x in range(len(rgb_data_list))]
+        depth_data_list = os.listdir(os.path.join(self.root_dir, "depth"))
+        depth_data_list = [os.path.join(self.root_dir, "depth", f"{x}.png") for x in range(len(depth_data_list))]
+        pose_data_list = os.listdir(os.path.join(self.root_dir, "pose"))
+        pose_data_list = [os.path.join(self.root_dir, "pose", f"{x}.txt") for x in range(len(pose_data_list))]
+
         return list(zip(rgb_data_list, depth_data_list, pose_data_list))
         
     
@@ -148,6 +155,9 @@ class ScannetDataset(RGBDDataset):
             for line in f:
                 intrinsics.append([float(x) for x in line.split()])
         intrinsics = np.array(intrinsics)
+        # Patch Note: resize the intrinsics to match the resized image size
+        intrinsics[0, :] /= self.resize_down_x
+        intrinsics[1, :] /= self.resize_down_y
         return intrinsics
     
     def _load_depth_intrinsics(self, path):
@@ -165,9 +175,10 @@ class ScannetDataset(RGBDDataset):
             for line in f:
                 intrinsics.append([float(x) for x in line.split()])
         intrinsics = np.array(intrinsics)
+        # Patch Note: skipped depth intrinsics resizing as ScanNet depth images are already in the desired resolution
         return intrinsics
 
-    def create__pcd(self, rgb, depth, camera_pose=None):
+    def create__pcd(self, rgb, depth, camera_pose=None, mask_img=False, filter_distance=np.inf):
         """
         This method should be implemented by subclasses to create a point cloud 
         from RGB-D images.
diff --git a/hovsg/graph/graph.py b/hovsg/graph/graph.py
index f4a81e9..a137ae3 100644
--- a/hovsg/graph/graph.py
+++ b/hovsg/graph/graph.py
@@ -128,7 +128,7 @@ class Graph:
         )
         self.sam.eval()
         # load the dataset
-        dataset_cfg = {"root_dir": self.cfg.main.dataset_path, "transforms": None}
+        dataset_cfg = {"root_dir": os.path.join(self.cfg.main.dataset_path, self.cfg.main.scene_id), "transforms": None, "resize": self.cfg.pipeline.rgb_resolution}
         if self.cfg.main.dataset == "hm3dsem":
             self.dataset = HM3DSemDataset(dataset_cfg)
         elif self.cfg.main.dataset == "scannet":
@@ -152,6 +152,9 @@ class Graph:
         # create the RGB-D point cloud
         for i in tqdm(range(0, len(self.dataset), self.cfg.pipeline.skip_frames), desc="Creating RGB-D point cloud"):
             rgb_image, depth_image, pose, _, depth_intrinsics = self.dataset[i]
+            # ensure same depth and rgb image size
+            if rgb_image.size != depth_image.size:
+                rgb_image = rgb_image.resize(depth_image.size)
             self.full_pcd += self.dataset.create_pcd(rgb_image, depth_image, pose)
 
         # filter point cloud
@@ -172,8 +175,6 @@ class Graph:
         frames_feats = []
         for i in tqdm(range(0, len(self.dataset), self.cfg.pipeline.skip_frames), desc="Extracting features"):
             rgb_image, depth_image, pose, _, _ = self.dataset[i]
-            if rgb_image.size != depth_image.size:
-                rgb_image = rgb_image.resize(depth_image.size)
             F_2D, F_masks, masks, F_g = extract_feats_per_pixel(
                 np.array(rgb_image),
                 self.mask_generator,
@@ -194,14 +195,31 @@ class Graph:
                 down_size=self.cfg.pipeline.voxel_size,
                 filter_distance=self.cfg.pipeline.max_mask_distance,
             )
+            # Patch Note: remove masks with less than 2 points to suppress ScanNet errors
+            assert len(masks_3d) == len(masks)
+            to_keep = list()
+            new_masks_3d = list()
+            for g in range(len(masks_3d)):
+                if len(masks_3d[g].points) > 1:
+                  to_keep.append(g)
+                  new_masks_3d.append(masks_3d[g])
+            masks_3d = new_masks_3d
+            F_masks = F_masks[torch.tensor(to_keep, dtype=torch.int, device=F_masks.device)]
             frames_pcd.append(masks_3d)
             frames_feats.append(F_masks)
             # fuse features for each point in the full pcd
             mask = np.array(depth_image) > 0
             mask = torch.from_numpy(mask)
+            if (mask.sum() < 1):
+                continue
             F_2D = F_2D[mask]
             # using cKdtree to find the closest point in the full pcd for each point in frame pcd
             dis, idx = tree_pcd.query(np.asarray(pcd.points), k=1, workers=-1)
+            if (idx >= n_points).sum() > 0:
+                # index == len(pcd) means not found
+                idx = idx[idx < n_points]
+                if idx.size < 1:
+                    continue
             sum_features[idx] += F_2D
             counter[idx] += 1
         # compute the average features
diff --git a/hovsg/labels/label_constants.py b/hovsg/labels/label_constants.py
index c27c96f..66e03c9 100644
--- a/hovsg/labels/label_constants.py
+++ b/hovsg/labels/label_constants.py
@@ -1,5 +1,91 @@
 """Label file for different datasets."""
 
+SCANNET_LABELS_NYU_40_WO_OTHERS = (
+    "wall",
+    "floor",
+    "cabinet",
+    "bed",
+    "chair",
+    "sofa",
+    "table",
+    "door",
+    "window",
+    "bookshelf",
+    "picture",
+    "counter",
+    "blinds",
+    "desk",
+    "shelves",
+    "curtain",
+    "dresser",
+    "pillow",
+    "mirror",
+    "floor mat",
+    "clothes",
+    "ceiling",
+    "books",
+    "refrigerator",
+    "television",
+    "paper",
+    "towel",
+    "shower curtain",
+    "box",
+    "whiteboard",
+    "person",
+    "night stand",
+    "toilet",
+    "sink",
+    "lamp",
+    "bathtub",
+    "bag",
+#     "otherfurniture",
+#     "otherstructure",
+#     "otherprop",
+)
+
+SCANNET_COLOR_MAP_NYU_40_WO_OTHERS = {
+    # 0: (174.0, 199.0, 232.0),
+    1: (152.0, 223.0, 138.0),
+    2: (31.0, 119.0, 180.0),
+    3: (255.0, 187.0, 120.0),
+    4: (188.0, 189.0, 34.0),
+    5: (140.0, 86.0, 75.0),
+    6: (255.0, 152.0, 150.0),
+    7: (214.0, 39.0, 40.0),
+    8: (197.0, 176.0, 213.0),
+    9: (148.0, 103.0, 189.0),
+    10: (196.0, 156.0, 148.0),
+    11: (23.0, 190.0, 207.0),
+    12: (247.0, 182.0, 210.0),
+    13: (219.0, 219.0, 141.0),
+    14: (255.0, 127.0, 14.0),
+    15: (158.0, 218.0, 229.0),
+    16: (44.0, 160.0, 44.0),
+    17: (112.0, 128.0, 144.0),
+    18: (227.0, 119.0, 194.0),
+    19: (82.0, 84.0, 163.0),
+    20: (163.0, 144.0, 156.0),
+    21: (125.0, 103.0, 91.0),
+    22: (222.0, 88.0, 99.0),
+    23: (190.0, 100.0, 230.0),
+    24: (60.0, 100.0, 200.0),
+    25: (45.0, 150.0, 120.0),
+    26: (243.0, 100.0, 20.0),
+    27: (158.0, 230.0, 70.0),
+    28: (132.0, 115.0, 200.0),
+    29: (189.0, 190.0, 200.0),
+    30: (45.0, 170.0, 85.0),
+    31: (210.0, 60.0, 150.0),
+    32: (255.0, 230.0, 120.0),
+    33: (210.0, 190.0, 90.0),
+    34: (255.0, 100.0, 150.0),
+    35: (180.0, 180.0, 210.0),
+    36: (230.0, 255.0, 100.0),
+    37: (150.0, 230.0, 180.0),
+    # 38: (60.0, 80.0, 200.0),
+    # 39: (170.0, 90.0, 220.0),
+}
+
 SCANNET_LABELS_20 = (
     "wall",
     "floor",
diff --git a/hovsg/utils/graph_utils.py b/hovsg/utils/graph_utils.py
index bc0aafc..8b3c443 100644
--- a/hovsg/utils/graph_utils.py
+++ b/hovsg/utils/graph_utils.py
@@ -502,6 +502,10 @@ def compute_3d_bbox_iou(bbox1, bbox2, padding=0):
     bbox1_volume = np.prod(bbox1_max - bbox1_min)
     bbox2_volume = np.prod(bbox2_max - bbox2_min)
 
+    # Patch Note: if either bounding box has zero volume, return IoU as 0
+    if bbox1_volume == 0.0 or bbox2_volume == 0.0:
+        return 0.0
+
     obj_1_overlap = overlap_volume / bbox1_volume
     obj_2_overlap = overlap_volume / bbox2_volume
     max_overlap = max(obj_1_overlap, obj_2_overlap)
